# 计算机组成原理

## 计算机如何识别010101？

计算机通过电压的高低来判断是0或者是1，不同的数据由不同的电信号组成。例如cpu和内存条上都有很多针脚，这些针脚就是用来发送和接收二进制数的通道。那么cpu和内存条此类的硬件如何交互呢？就是通过主板上的印刷电路，可以理解为嵌在电路板上的一根根电线。通过电线传输二进制的0和1，也就是传输高低电平的0和1。电路板上有很多条线路，即可以同时传输多个二进制数位，每个二进制数称为1 bit。

## 数字、文字、图像如何使用二进制表示？

## CPU如何对二进制数进行加减乘除？

## 如何存储这些二进制数？

## 如何从内存中取出想要的数据？

# 计算机系统概述

计算机由硬件和软件构成，硬件就是cpu、显示器、鼠标，软件就是看得见但是摸不到的东西，例如操作系统、微博等。

## 硬件的发展

### 电子管时代

第一台电子数字计算机ENIAC(1946)，逻辑元件使用了电子管，逻辑元件就是用来处理电信号的最小基本单元。即用线路把很多的逻辑元件连接起来，从而用电路来运算。这个时候程序员使用穿孔纸带进行编程，有孔表示0，没孔表示1。

### 贝尔实验室，晶体管替代电子管

使用晶体管替代电子管，晶体管相较于电子管更小。这个时候一台计算机需要几万个到几十万晶体管，需要将这些晶体管用手工的方式将它们焊接到电路板上。几十万个还是要消耗大量时间，所以这个时间段的计算机硬件还是十分不可靠的。可能一个焊点出错会导致整个计算机不可用。

### 中小规模集成电路时代

将元件集成在一个芯片上。使得电脑更小，可靠性提高 。这个时候出现了很多编程语言。开始有了分时操作系统。

### 大规模和超大规模集成电路

微处理器，微型计算机。这个时候也是个人计算机的萌芽阶段，计算机不仅仅是给科学和工业提供使用，走进了千家万户。指甲盖大小的cpu有80多亿个晶体管。

## 机器字长

机器字长代表了一个cpu一次整数运算能够处理的二进制位数，intel最早的cpu 8080一次只能8位，现在都是64位。很显然8080对16个二进制位的加法需要两次计算，而64位的cpu只需要一次整数。

## 摩尔定律

揭示了信息技术进步的速度，集成电路上可容纳的晶体管数目，约每隔18个月便会增加一倍，整体性能也将提升一倍

## 软件的发展

### 机器语言

使用010101进行编程，但是由于可读性很差，所以出现了下面的汇编语言

### 汇编语言

本质上和机器语言相同，但是汇编会将机器语言转换为人类更容易明白的符号。在这一阶段，程序员编写软件是一个很复杂的工作。

### 高级语言

c++、pascal 在这个阶段程序员无需关注机器的具体特性是什么，只需要专注自己要解决的问题即可。 

### 操作系统

DOS、MicroSoft、Android、iOS

# 计算机硬件的基本组成

## 早期的冯诺依曼机的结构

ENIAC的每个指令都需要操作员接线缆的方式告诉计算机。虽然这台计算机的速度很快，但是由于操作员接线来告诉计算机下一步怎么做。所以很高的效率就被这些时间给抵消了。为了解决这个问题，冯诺依曼就提出了存储程序的构想，将指令以二进制代码的形式事先输入计算机的主存储器。然后计算机会按照存储器中的一系列指令执行下去。

这种实现方式，可以使得我们将想要让计算机执行的一系列指令告诉计算机，由计算机自己一条条执行，而不需要操作员告诉他。这台计算机也是世界上首个使用冯诺依曼架构的计算机，即EDVAC 

使用输入设备将信息转换为机器能够识别的形式，即010101。处理之后流向运算器，通过运算器中转将程序数据放到存储器中。存储器就是用来存放我们要处理的数据和程序指令的。而运算器最主要的作用就是进行算术运算、逻辑运算。经过运算器的处理之后，结果会通过输出设备转换为我们人类熟悉的形式。控制器会使用电信号协调其他部件相互配合的工作。控制器也会解析存储器中存储的指令。控制器从存储器中解析加法或者减法的指令，然后指挥运算器执行相应的操作。所以控制器就是用来指挥程序运行的。

对于乘法运算，可以设计一个专门的硬件电路来实现乘法运算。也可以使用软件的方式。执行多次加法运算来实现。硬件更快成本也高，软件较慢，成本也低。

### 冯诺依曼计算机的特点

1. 计算机由五大部件组成
2. 指令和数据以同地位存于存储器，按地址寻访
3. 指令和数据用二进制表示
4. 指令由操作码和地址码组成
   1. 操作码：这条指令需要进行什么样的操作
   2. 地址码：要操作的数据存放在内存的什么地址当中
5. 存储程序，预先将指令和数据存储到存储器中。
6. 以运算器为中心，运算器处于最中心的位置，作为一个中转。计算机本质上就是一个加工工厂

## 现代计算机的结构

通常以存储器作为核心，并且由于运算器和控制器是紧密联系的控件，所以在现代计算机的结构中，这两者通常被继承到同一个芯片上，即cpu=运算器+控制器

# 各个硬件的工作原理

## 主存储器

主存储器中有一个`存储体`、`MAR`,`MDR`。

- MAR：Memory Address Register 存储地址寄存器
- MDR：Memory Data Register 存储数据寄存器
- 存储体：存储数据

寄存器是什么呢？寄存器也是用来存放二进制数据的，只不过MAR存放的是地址相关的数据，MDR存储的是数据相关的数据。

### 存储元

用于存储二进制数据的电子原件，使用电容的原理制造的，使用电容可以存储电荷，所以可以保存二进制的比特位，多个存储元就构成了一个存储单元。

### MAR

数据是以二进制的形式存储在存储体中的，存储体在内存中会被分为一个个的存储单元，每个存储单元会存放一串二进制代码，这串二进制代码我们称为存储字。

每个存储字包含的多少个二进制位，我们称之为存储字长。每个存储单元对应一个地址信息从0开始的，这个地址信息就是MAR中应该指明的信息。如果要读取的是2号存储单元，那么CPU就应该往MAR寄存器中写入2号的信息。

MAR中指明了要访问哪一个存储单元，指明了存储单元的地址。所以MAR的位数，就直接反映了存储体中到底有多少个存储单元。因为每个存储单元都在MAR中。另一方面，从存储单元中取出来的数据是要放到MDR中的。所以MDR的位数要和MAR保持一致。即=存储字长。

例如：MAR=4位，说明总共能存储2的4次方个存储单元，因为4个二进制位，最多也就能报是这么多数字。

如果MDR=16位，每个存储单元能够存放16bit，1个字长=16bit。

## 运算器

运算器就是用来实现算术运算、逻辑运算

- ACC	累加器，用来存放操作数或运算结果
- MQ      乘商[除]寄存器，在乘除运算时，用来存放操作数或者运算结果
- X          通用的操作数寄存器，用于存放操作数
- ALU     算术逻辑单元，通过内部复杂的电路实现算术运算，逻辑运算

## 控制器

- CU：控制单元，分析指令，给出控制信号
- IR：指令寄存器，存放当前执行的指令
- PC：程序计数器，存放下一条指令地址，有自动加1功能。

### 完成一条指令

1. 取指令 PC
2. 分析指令 IR
3. 执行指令 CU

# 计算机的工作过程

```c
int a = 2,b=3,c=1,y=0;
void main(){
    y =a*b+C
}
```

![](https://ooo.0o0.ooo/2021/01/08/VuWLUqkX8jI25ys.png)

将高级语言翻译为机器语言

程序执行之前，PC指向0号主存地址的位置，接下来要将这个位置的指令取出来执行，PC存放的内容，需要通过地址总线传送到MAR即地址寄存器中，这个时候PC是0，这个时候就会导致MAR寄存器的值变为0，即控制器向主存指明了接下来要访问的是0号地址对应的数据，同时控制器会通过控制总线告诉主存储器这次进行的是读操作，主存储器会根据MAR记录的地址信息去存储体中寻找0号地址对应的数据，并且将这些数据放到MDR即数据寄存器中，所以MDR中就存放了0对应的指令。这条指令会通过数据总线放到IR中，即指令寄存器中。这就会造成控制器中有了本次要执行的指令。

这条指令的前六位操作码即这条指令要进行什么操作，会被从到CU控制单元中，CU进行分心就会发现这是一条取数的指令，然后将地址码即操作码后面的地址码对应的内存单元中的数据取出来，并且放到ACC寄存器中。由于这里有取数的操作，所以要将地址码放到MAR中，地址码对应的是5号单元，MAR去存储体中寻找5号单元的数据，找到后放到MDR中，即a=2，在控制单元的指挥下MDR的数据会被传送到ACC累加寄存器中。这就是整个取数的指令。这个数的值就被放到ACC寄存器中了。

取指令之后，PC的值会自动加1

