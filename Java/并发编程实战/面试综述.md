# Synchronized的底层原理

Synchronized的原理是和jvm指令和monitor有关系的。假设在代码中使用了Synchronized关键字，那么在底层编译后的jvm指令中会出现monitorenter和monitorexit两个指令

````java
monitorenter
//代码对应的指令
monitorexit
````

因为每个对象都会对应一个monitor对象，一个类对象也对应一个monitor，如果要对这个对象加锁，那么必须获取这个对象关联monitor的lock锁。

在monitor中有一个计数器，默认为0，每当有一个线程想要获取对象的锁，会去看monitor是否是0，如果是0就说明没有枷锁，那么将0改为1，表明该线程获取了该对象的锁。

例如又来了一个线程想要获取该对象的锁，这个时候新的线程就会发现对象的monitor对象计数器为1。所以线程1加锁成功，线程2加锁失败，进入阻塞等待的状态。

为什么说synchronized是可重入的？

```java
public class Widget {
    public synchronized void doSomething() {
        System.out.println("方法1执行...");
        doOthers();
    }
    public synchronized void doOthers() {
        System.out.println("方法2执行...");
    }
}
```

可重入加锁的意思就是：当一个线程拿到一个对象锁，执行了一段代码之后，释放了该锁。类似上面这种情况，在doSomething中拿到了该对象的锁，那么进入内层的方法doOthers的时候，还可以继续使用之前的锁执行，区别就是将monitor的计数器+1。也叫递归锁，以此类推。当该线程从里面的方法释放的时候就会减去计数器的值直至减到0。

## Synchronized的锁升级

# CAS的理解和底层实现原理

例如多个线程访问同一个数据，例如要访问一个map`HashMap map = new HashMap`

可能要使用到synchronized

```java
synchronized(map){
    //对map进行相关操作
}
```

## CAS是什么？

假设有如下代码被多线程执行

```java
public class MyObject{
    int i = 0;
    public synchronized void increament(){
        i++;
    }
}

MyObject object = new Object();
object.increament();
```

此时synchronized就是相当于对myObject进行加锁，同一时间多个线程执行的情况下只有一个线程能够获取该对象的锁。这样的情况下就能保证现成的安全性。

但是这样就是有一个问题，多线程情况下，执行该串代码没相当于是串行执行了该代码。效率不是很高。就像是排队执行。于是有了下面的方式

```java
public class MyObject{
	AtomicInteger i = new AtomicInteger;
    public void increament(){
        i++;
    }
}
```

此时这个时候多线程执行这串代码，不需要synchronized也可以保证线程安全性。`AtomicInteger`就是juc包下的原子类，基于CAS来实现的。

假设现在有一个线程对AtomicInteger进行累加操作，那么AtomicInteger会先读取当前的值，即当前值为0，然后对0进行累加++，即0变为1。然后我们要设置值将1设置给i.

![](https://i.loli.net/2020/12/17/TAPXHkWy37th8R6.png)

![](https://i.loli.net/2020/12/17/gWKrTsciAjkH1aL.png)

CAS在底层保证了一定是原子的，同一时间只有一个线程可以执行CAS，先比较再设置，其他的线程CAS的时候会失败 

```java
cas(value,Expect,newVlaue);
    
if(value == Expect){//说明没有线程修改
    value = newValue;
} else {
    try again or fail//再来一次，自旋一次。
}

```

## CAS有什么缺点

ABA、自旋时长、只能保证一个共享变量原子操作

# ConcurrentHashMap实现线程安全的底层原理

假设多条线程对集合进行put操作。

```java
HashMap map = new HashMap
synchronized(map){
    map.put("", "")
}
```

假设两个线程同时往一个位置put的情况下需要做同步，否则如果直接加锁会影响性能。在juc包就有一个ConcurrentHashMap，无需加锁直接使用。

## JDK1.7之前，分段加锁

[数组1]，[数组2]，[数组3]

假设多线程操纵的数组不同，那么只会锁对应的数组而不是全部都锁，即分段加锁

## JDK1.8及之后

jdk1.8之后又变为大的数组，数组中的每个元素进行put操作。都是有一个不同的锁，假设两个线程都put到5这个位置，那么就会使用CAS的策略。即保证线程并发的安全性。同时间只有一个线程能够执行成功。

分段加锁，通过对数组每个元素执行CAS策略，如果是很多线程对数组中不同元素执行put，是没有关系的。如果其他的人失败了。那就会基于这个位置进行链表+红黑树进行处理

如果是对数组中同一个位置元素进行操作，才会加锁串行化处理，如果不是同一个位置额度元素操作，此时大家可以并发运行。

# AQS[Abstract Queue Synchronizer]实现原理

多线程访问同一个共享数据可以使用synchronized、Lock、CAS这些方式来保证共享数据的线程安全。

ReentrantLock底层是使用AQS来实现的。

```java
ReentrantLock lock = new ReentrantLock();
lock.lock();

lock.unlock();
```

![](https://i.loli.net/2020/12/18/8dXPV5Jxjf2WuMR.png)

默认情况下，AQS使用的策略是非公平锁，比如加锁线程为线程1，线程1去唤醒等待队列的队头线程2，线程2还没来得及加锁的时候来了个线程3。线程3二话不说就执行CAS，更新state=1并且加锁成功，这个时候线程2又进行修改，修改失败，继续呆在等待队列中，这就是非公平锁。但是公平或者非公平是可以参数设置的。非公平锁只能进入等待队列中，线程3来的时候发现队列中有线程2就直接进入等待队列等待公平锁就是获取锁前判断一下队列

# 线程池的底层工作原理

系统是不可能让我们无限制的创建很多线程的，因为线程的创建和销毁都会浪费资源。所以一般都会创建一个线程池，有一定数量的线程，让他们执行各种各样的任务。每个线程执行完成任务之后，不会销毁自己，而是回到线程池，继续等待执行任务的机会。

## 简单的使用方式

```java
ExecutorService pool = new Executors.newFixdThreadPool(10)
pool.submit(
new callable(){
    public void run(){
        //任务
    }
}
);
```

![](https://i.loli.net/2020/12/18/aWOqznDEdBVNQt5.png)

FixdThreadPool是无界阻塞队列，即线程池只有三个线程，但是队列是无限大小的，只要线程池的线程忙不过来，那么新的任务都会加到队列中。可能任务会越来越多

## 线程池的核心配置参数，应该怎么用？

```java
return new thredPoolExecutor(nThreads,
nThreads,
0L,
TimeUnit.MILLISECONEDS,
new LinkedBlockingQueue<Runnable>());
```

代表线程池的类是ThreadPoolExecutor。创建一个线程池是需要`corePoolSize`、`maximumPoolSize`、`keepAliveTime`、`queue`这几个参数，如果我们不使用fixed之类的线程池，自己完全可以通过这个构造函数创建自己的线程池。

- corePoolSize:3
- maximumPoolSize:Interger.MAX_VALUE
- keepAliveTime:60s
- queue:new ArrayBlockingQueue<Runnable>(200)//队列中只能放200个任务

刚开始进入线程池是空的，提交任务的时候，如果线程数量少于corePoolSize，那么会创建线程直接去执行任务。如果corePoolSize=3，那么提交三个任务就会创建三个线程。只要线程数量达到corePoolSize，那么后续的任务再来直接会进入队列中。例如我们的队列是上面的那种就是只会创建200个任务。

但是如果我们的队列满了，放不进去了。corePoolSize线程还在处理任务，这个时候我们可以创建一些额外的线程出来。创建多少个呢？就是由我们的`maximumPoolSize`设置的值来决定的，上面设置的最大是Integer的MAX_VALUE，但是如果额外的线程加上线程池线程将队列中的所有任务都处理完了，没事做了。这个时候额外线程就会根据空闲时间`keepAliveTime`间隔后自动销毁。

## Reject策略

那假设上面的队列、额外线程、核心线程都是满的呢？这个时候就要`Reject策略了`可以传入RejectExcutionHandler即如下：

1. AbortPolicy				抛出异常
2. DiscardPolicy             把你的任务扔了
3. DiscardOldestPolicy 抛弃旧的任务
4. CallerRunsPolicy        
5. 自定义

要根据上述原理去制定自己的线程池，考虑到corePoolSize的数量，队列类型，最大线程池数量，拒绝策略，线程释放时间。

一般比较常用的是Fiexed线程池，corePoolSize和maximumPoolSize是一样大的，等待时间是0，队列是无界的。也就是说，线程池中只会有corePoolSize那么多的线程，但是只要他们繁忙，就可以无限制的往里面放任务。队列永远不会满。

还有一种方法，就是将队列有界，并且将额外线程数量设置为Inter的最大值。这种策略很怕瞬间任务很多corePoolSize和队列都满了，这个时候就会无限制的创建额外线程。

## 如果在线程池中使用无界阻塞队列会怎么样？

面试题：在远程服务异常的情况下，使用无界阻塞队列，是否会导致内存飙升？

即我们的线程调用远程服务，调用失败的话经常会超时，会导致服务卡住。那么这个线程会等待远程服务，线程处理速度很慢，队列又是无界的，任务会越来越多。如果队列越来越大。会导致内存飙升，还可能导致OOM内存溢出。

## 线程池的队列满了之后，会发生什么事吗？

corePoolSize设置为10，maxinumPoolSize设置为Integer最大值，队列设置为ArrayBlockingQueue(200)

在这样的一个情况之下，有界队列能够避免内存溢出，但是可能会导致队列被塞满，这个时候就要说清楚keepAlive和maxinumPoolSize的机制了。可以说是设置额外线程，keepAlive过了会自动销毁，或者可能短时间创建大量线程等。但是每个线程都有自己的栈内存，占用一定的内存资源，导致内存耗尽，系统也会崩溃掉。

即使内存没有崩溃也会导致CPU负载过高，系统崩掉。但是如果设置了maxinumPoolSize为200，万一有全部都满了，可能会有任务被reject掉。

任务很多的情况下可以使用无界队列，相反可以使用有界+maxinumPoolSize的方式，当然要设置一定的最多线程数量。还可以设置reject策略，如果线程池无法执行更多的任务了，此时可以把这个任务信息持久化写入到磁盘中，后端专门启动一个线程，后续等待线程池的工作负载降低了，再从磁盘中读取持久化的任务重新执行。

## 如果线上机器宕机，线程池的阻塞队列请求怎么办?

必然会导致线程池中积压的任务实际上来说都会丢失的。如果要提交一个任务到线程池中，在提交之前，可以现在数据库中插入这个任务的信息，更新状态：已提交，未提交，已完成。提交成功后，更新他的状态是已提交的状态。假设宕机了之后，系统重启扫描数据库中的未提交和已提交状态的任务，可以把任务的信息读取出来，重新提交到线程池中去。

# JMM(Java内存模型)

## 执行流程

![](https://i.loli.net/2020/12/18/UMc1toIuVNzlAmv.png)

## 可见、有序、原子

### 可见

保证多线程操作一个共享变量的时候，线程1的修改对变量的操作是对线程2可见的。

### 原子

原子性：data++，必须是独立执行的，没有人影响我的，一定是我自己执行成功之后，别人才能进行下一次data++的操作。整个操作过程是不能分割的。

### 有序

对于代码，有序性要摒弃编译器优化，同时的指令重排序，因为重排序可能会导致代码出现问题。

## 聊聊volatile的底层原理

说清楚volatile必须先说清楚并发三个特性。

volatile可以解决可见性的问题，即线程的工作内存保存了主内存值缓存，后续的操作都是对线程内自己的缓存进行操作并将值设置给主内存，但是线程内的值可能是过时的，这样就因为可见性导致了数据不一致的问题。如果我们对共享变量使用了volatile，那么这个时候线程1操纵某个数据的时候，会将设置的值重新刷回主存中，并且会通知线程2的工作内存中缓存的变量数据且将其设置为失效的状态，当线程2访问工作内存的数据的时候，发现失效了，就知道要去主内存中重新拿新的数据进行操作。

## volatile如何保证有序性

在java中的happens before原则，在一定程度避免有序性的问题。规则约定了不允许编译器在某种情况下对指令进行重排，必须保证代码的有序性。

volatile标注的变量，对变量的读写操作不会跟前后代码进行重排，并且当volatile变量被修改之后会通知线程中的缓存标记线程中的缓存变量为失效，线程再去拿自己的缓存变量发现失效了就去主存重新拿新的变量值。

volatile+原子性：volatile不能保证原子性，某些极端情况下能够保证原子性。

## volatile使用了内存屏障，那么内存屏障是什么？

给一个变量加上volatile，那么这个变量前后会带上各种屏障，那样就不会进行指令重排。

具体地说就是volatile修饰的变量，执行写操作的话，JVM会加上一条lock前缀指令给CPU，CPU在计算完成之后会立即将这个值写回主内存，同时因为有MESI缓存一致性协议，所以各个CPU都会对总线进行`嗅探`，自己本地缓存中的数据是否被别人修改。

如果发现被修改了，就会将缓存中的变量失效掉，强制从主内存read、load、use再次加载。

lock前缀指令+MESI缓存一致性协议

禁止指令重排就像synchronized底层一样，会在修饰的变量前后加上load、store或者类似其他的内存屏障指令，加上指令之后就禁止了指令重排。

# SpringIOC&AOP理解

spring最关键的两个机制就是IOC和AOP

## 动态代理

定义切面切点，将方法织入。

```java
public class ProxyServiceA {
    private MyServiceA serviceA;
    
    public void doServiceA{
        //开启事务
		..调用serviceA方法
        myservice.dosomthing();
        //关闭事务或回滚事务
    }
}
```

spring动态生成一个代理类，将你的类注入进去。

```java
@Controller
public class Controller{
    @Autowired
    private MyServiceA serviceA
        
    public void doSomething(){
        serviceA.adsda();
    }
}
调用serviceA的时候会先生成一个代理类，使用代理类调用代理类中的方法，执行事务，然后执行serviceA的方法。
```

# CGlib和动态代理的区别

# spring的bean是否是线程安全的

否定的，spring的bean默认是单例的，假设单例bean中有一个共享变量++，多个线程操作一个实例的时候，没有进行加锁或者volatile处理的时候，可能会造成数据错乱。

但是并不会造成什么问题，因为多线程并发访问的是实例组件，而不是内存中的共享数据。大部分情况是并发访问数据库。

# spring的事务实现原理是什么，事务传播的机制是什么？

事物的实现原理即@Transationl注解，spring会使用AOP思想，对我们的方法在执行之前，先去开启事务，执行完毕之后，根据方法是否报错，来决定回滚还是提交事务。

传播机制：假设方法A和方法B加上了事务注解，当多个方法都加上了事务注解的时候，执行的时候事务如何传播。有很多个级别。

1. REQUIRED 如果没有事务，就创建一个新的，如果有了就加到存在的事务中。即上面的举例只会执行一个事务，
2. SUPPORTS 如果当前存在事务，就加入该事务，否则以非事务运行。
3. MANDATORY 
4. ...

# SpringBoot核心架构

## 自动装配

引入starter后即可....